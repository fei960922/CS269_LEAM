{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from main import main\n",
    "from src.model import *\n",
    "from src.utils import *\n",
    "from src.trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAM \n",
    "1-layer embeding with label attention, 1-layer mlp for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0 ---\n",
      "[0/100] loss_epoch : 0.29 val_match : 0.0824 match_epoch : 0.1472 val_hs : 0.2105 hs_epoch : 0.0724\n",
      "--- epoch: 10 ---\n",
      "[10/100] loss_epoch : 0.03 val_match : 0.0882 match_epoch : 0.5805 val_hs : 0.2050 hs_epoch : 0.5297\n",
      "--- epoch: 20 ---\n",
      "[20/100] loss_epoch : 0.02 val_match : 0.1892 match_epoch : 0.7681 val_hs : 0.5291 hs_epoch : 0.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/reuters'\n",
    "vocab_url = os.path.join(data_dir, 'vocab.pkl')\n",
    "train_url = os.path.join(data_dir, 'train')\n",
    "test_url = os.path.join(data_dir, 'test')\n",
    "vocab = pickle.load(open(vocab_url, 'rb'))\n",
    "vocab = list(zip(*sorted(vocab.items(), key=lambda x: x[1])))[0]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "train_set, train_labels, class_names = dataset(train_url)\n",
    "test_set, test_labels, _ = dataset(test_url)\n",
    "\n",
    "model = Leam_Classifier(vocab_size, len(class_names), 256, 256, 10, \n",
    "                        n_layer=1, dropout_rate=0.8, embpath=None, label_att=True, multilabel=True)            \n",
    "trainer = Trainer(batch_size=128, num_epoches=100, learning_rate=1e-3, valid_freq=10, model_type='embed')\n",
    "\n",
    "trainer.set_vocab(vocab)\n",
    "trainer.set_validation(test_set, test_labels)\n",
    "trainer.init_model(model)\n",
    "train_prob, train_beta = trainer.fit(train_set, train_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation (test)\n",
    "val_prob, val_beta = trainer.predict(test_set)\n",
    "val_pred = val_prob > 0.5\n",
    "# test not seen in training (task)\n",
    "task_url = os.path.join(data_dir, 'task')\n",
    "seq_task, seq_labels, _ = dataset(task_url, monitor=False)\n",
    "test_prob, test_beta = trainer.predict(seq_task)\n",
    "test_pred = test_prob > 0.5\n",
    "multilabel_eval(seq_labels, test_pred, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shwo results and highlights\n",
    "trains = zip(train_prob>0.5, train_labels, train_set, train_beta)\n",
    "vals = zip(val_pred, test_labels, test_set, val_beta)\n",
    "tests = zip(test_pred, seq_labels, seq_task, test_beta)\n",
    "\n",
    "def write(cache, name, max_len=80, imp=1.5e-2):\n",
    "    f.write('<p style=\"background-color:green;\">%s</p>'%name)\n",
    "    for pred, true, sent, recon in cache:\n",
    "        if (pred[true==1] != 1).all():\n",
    "            f.write('<p style=\"background-color:red;\">All Miss</p>')\n",
    "        elif (pred != true).any():\n",
    "            f.write('<p style=\"background-color:blue;\">Partial Wrong</p>')                    \n",
    "        f.write('<p>prediction: {}, true: {}</p>'.format(class_names[pred==1], class_names[true==1]))\n",
    "        f.write('<p>')\n",
    "\n",
    "        f.write('<p>')\n",
    "        for i, word in enumerate(sent):\n",
    "            if i >= max_len:\n",
    "                break\n",
    "            if beta[i] > imp:\n",
    "                f.write('<mark class=\"red\">{}</mark> '.format(vocab[word]))\n",
    "            else:\n",
    "                f.write('{} '.format(vocab[word]))                \n",
    "        f.write('</p>')    \n",
    "        f.write('<HR SIZE=5>')\n",
    "\n",
    "        \n",
    "with open('res.html', 'w', encoding='gbk') as f:\n",
    "    write(tests, 'Test', max_len=80, imp=1.5e-2)\n",
    "    write(vals, 'Validation', max_len=80, imp=1.5e-2)\n",
    "    write(trains, 'Train', max_len=80, imp=1.5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding \n",
    "1-layer embeding without label attention, 2-layer mlp for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/reuters'\n",
    "vocab_url = os.path.join(data_dir, 'vocab.pkl')\n",
    "train_url = os.path.join(data_dir, 'train')\n",
    "test_url = os.path.join(data_dir, 'test')\n",
    "vocab = pickle.load(open(vocab_url, 'rb'))\n",
    "vocab = list(zip(*sorted(vocab.items(), key=lambda x: x[1])))[0]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "train_set, train_labels, class_names = dataset(train_url)\n",
    "test_set, test_labels, _ = dataset(test_url)\n",
    "\n",
    "model = Leam_Classifier(vocab_size, len(class_names), 256, 256, 10, \n",
    "                        n_layer=2, dropout_rate=0.8, embpath=None, label_att=False, multilabel=True)            \n",
    "trainer = Trainer(batch_size=128, num_epoches=100, learning_rate=1e-3, valid_freq=10, model_type='embed')\n",
    "\n",
    "trainer.set_vocab(vocab)\n",
    "trainer.set_validation(test_set, test_labels)\n",
    "trainer.init_model(model)\n",
    "train_prob, train_beta = trainer.fit(train_set, train_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_url = os.path.join(data_dir, 'task')\n",
    "seq_task, seq_labels, _ = dataset(task_url, monitor=False)\n",
    "test_prob, test_beta = trainer.predict(seq_task)\n",
    "test_pred = test_prob > 0.5\n",
    "multilabel_eval(seq_labels, test_pred, full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of word\n",
    "2-layer mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/reuters'\n",
    "vocab_url = os.path.join(data_dir, 'vocab.pkl')\n",
    "train_url = os.path.join(data_dir, 'train')\n",
    "test_url = os.path.join(data_dir, 'test')\n",
    "vocab = pickle.load(open(vocab_url, 'rb'))\n",
    "vocab = list(zip(*sorted(vocab.items(), key=lambda x: x[1])))[0]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# bow\n",
    "train_set, train_labels, _, class_names = bow_dataset(train_url, vocab_size)\n",
    "test_set, test_labels, _, _ = bow_dataset(test_url, vocab_size)\n",
    "\n",
    "model = Classifier(vocab_size, len(class_names), n_layer=2, n_hidden=256, dropout_rate=0.8)\n",
    "trainer = Trainer(batch_size=128, num_epoches=100, learning_rate=1e-3, valid_freq=10, model_type='bow')\n",
    "\n",
    "trainer.set_vocab(vocab)\n",
    "trainer.set_validation(test_set, test_labels)\n",
    "trainer.init_model(model)\n",
    "train_prob = trainer.fit(train_set, train_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_url = os.path.join(data_dir, 'task')\n",
    "seq_task, seq_labels, _, _ = bow_dataset(task_url, vocab_size, monitor=False)\n",
    "test_prob, test_beta = trainer.predict(seq_task)\n",
    "test_pred = test_prob > 0.5\n",
    "multilabel_eval(seq_labels, test_pred, full=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
